\section{Python Interpreters}\label{sec:pythoninterpreters}
%%% note : add this to the section of python interpreters 
Due to the lack of support for most non-conventional Python interpreters, we mainly focus on micro-benchmarks.
Except for \textsc{PyPy}, most of the Python implementations do not support extra Python libraries, despite those extra implementations being developed to optimize a specific library, such as \textsf{Numba} with \textsf{Numpy}, or \textsf{intelpython} with machine learning algorithms.

\paragraph{Preliminary studies}
For the first studies, we used the official version of Python, because the goal was mainly to highlight the impact of the structure of code on energy consumption.
One main drawback of the previous method is the work to be done to update the existing code base to reduce  energy consumption.
To avoid such hustle, we tried to find a non-intrusive approach to make the Python code more eco-friendly without altering its structure.
Python is an interpreted language, which led many initiatives to implement their own interpreter to improve one or many aspects of the Python code.
In the following section, we discuss the impact of those implementations on the energy consumption of python programs, and in which case, one should use a non-conventional interpreter to save the energy consumption of their application.

To do so, we gathered a list of interpreters, transpilers and other optimization libraries that can contribute to reduce the energy consumption of legacy Python applications:
\begin{enumerate}
    \item \textbf{CPython}:\footnote{\url{https://www.python.org/}}
          This Python interpreter, written in C, is the reference interpreter of Python.
          CPython compiles the source code into byte-code and then interprets it.
          The CPython project supports both versions of Python 2 and 3;
    \item \textbf{PyPy}:\footnote{\url{http://pypy.org}}
          An alternative implementation of the Python interpreter.
          It is written using \emph{RPython} to use the JIT.
          It compiles the most used portions of the Python code into a binary code for better performance.
          To benefit from these optimizations, the program has to be executed for at least for few seconds so the JIT has enough time to warm up, the JIT optimization are only applied to the code written by the developer and not to external libraries;
    \item \textbf{Cython}:\footnote{\url{https://github.com/cython/cython}}
          A static compiler for Python.
          It translates the Python code into C, and then compiles it using a C compiler.
          It also supports an extended version of the Python language that allows programmers to call \emph{C functions}, declare \emph{C types} and use static types, which will help the translation of Python objects into native types, such as integers, float.
          This often means better performances, since native C libraries are almost all the time faster than the Python written once~\cite{pereira_energy_2017};
    \item \textbf{Intel\,Python}:\footnote{\url{https://software.intel.com/en-us/distribution-for-python}}
          A customized interpreter developed by Intel to enhance performances of Python programs.
          It is dedicated to data sciences and high-performance computing.
          It uses some Intel kernel libraries, such as Math Kernel Library (Intel MKL\footnote{\url{https://software.intel.com/en-us/mkl}}) and data analytics acceleration library (Intel DAAL\footnote{\url{https://software.intel.com/en-us/intel-daal}}).
          It supports both versions of Python;
    \item \textbf{Active\,Python}:\footnote{\url{https://www.activestate.com/products/activepython/}}
          It is developed by the Activestates company and provides a standardized Python distribution to ensure license compliance, security, compatibility and performance.
          Therefore, ActivePython implements its built-in packages (more than 300 packages) and supports both versions of Python;
    \item \textbf{IronPython}:\footnote{\url{https://ironpython.net}}
          A .Net-based Python interpretation platform written in C\# that is used with the .Net virtual machine or Mono.
          It benefits from all the optimizations of .Net virtual machines, such as the JIT and garbage collector mechanisms;
    \item \textbf{GraalPython}:\footnote{\url{https://github.com/graalvm/graalpython/}}
          A Python interpreter that is based on GraalVM\footnote{\url{https://www.graalvm.org/docs/why-graal/}} (a universal virtual machine developed by oracle for running applications written in different programming languages).
          For the time being, it only supports Python~3 and it is still in the experimental stage;
    \item \textbf{Jython}:\footnote{\url{https://jython.github.io}}
          An implementation of Python programming language written in Java for the \emph{Java Virtual Machine} (JVM).
          Similar to IronPython and GraalPython, it leverages the optimization mechanisms provided by the JVM to enhance the Python performances;
    \item \textbf{MicroPython}:\footnote{\url{http://micropython.org}}
          A lightweight Python version dedicated to embedded systems and micro-controllers;
    \item \textbf{Nuitka}:\footnote{\url{http://nuitka.net/pages/overview.html}}
          A Python compiler written in Python that generates a binary executable from Python code.
          It translates the Python code into a C program that is then compiled into a binary executable;
    \item \textbf{Numba}:\footnote{\url{https://numba.pydata.org}}
          A library that includes JIT compiler to enhance the performances of Python functions using the industry-standard LLVM compiler library;
    \item \textbf{Shedskin}:\footnote{\url{https://github.com/shedskin/shedskin}}
          A static transpiler that translates implicitly statically typed python into C++ code;
    \item \textbf{Hope}~\cite{akeret_hope_2015}:
          A Python library that aims to introduce JIT compiler into the Python code;
    \item \textbf{Parakeet}~\cite{DBLP:conf/hotpar/RubinsteynHWS12}:
          A runtime accelerator for an array-oriented subset of Python;
    \item \textbf{Stackless\,Python}:\footnote{\url{https://github.com/stackless-dev/stackless/wiki}}
          An interpreter that focuses on enhancing multi-threading programming;
    \item \textbf{Pyjion}:\footnote{\url{https://github.com/microsoft/pyjion}}
          A JIT API for CPython, same purpose as Parakeet and Hope;
    \item \textbf{Pyston}:\footnote{\url{https://blog.pyston.org}}
          A performance-oriented Python implementation built using LLVM and modern JIT techniques.
          The project is funded by Dropbox;
    \item \textbf{Grumpy}:\footnote{\url{https://github.com/google/grumpy}}
          A source-to-source transpiler that translates the Python code into Go before being compiled to a binary executable.
          It also offers an interpreter, called \emph{grumprun}, which can directly execute the Python code.
          Unfortunately, we cannot use it because the project is already outdated (last commit is in 2017) and it has a lot of limitations in terms of supporting the Python language, such as some built-in functions and standard libraries;
    \item \textbf{Psyco}:\footnote{\url{http://psyco.sourceforge.net}}
          A JIT compiler for Python;
    \item \textbf{Unladen\,Swallow}:\footnote{\url{https://unladen-swallow.readthedocs.io/en/latest/}}
          An attempt to (use) LLVM as a JIT compiler for CPython.
\end{enumerate}


\subsection{Runtime Classification}
Before further proceeding with the list of candidate runtime for Python applications, we propose a classification according to several criteria:
\begin{description}
    \item[\bf Type] refers to the category of runtime infrastructure that supports the execution of a Python application.
        In particular, we consider 3 types of environments: \emph{Interpreter}, \emph{Compiler} and \emph{Library};
        \emph{Interpreter} refers to the class of environment that does not require any preprocessing of Python source code;
        \emph{Compiler} introduces a compilation phase before the execution of the application.
        Finally, \emph{Library} requires some modification of the source code;
    \item[\bf Runtime] refers to the technology supporting the execution of a Python application.
        This technology can refer to the programming language used to program the interpreter, the target language for a compiler or a library;
    \item[\bf JIT optimization] refers to the support of \emph{just-in-time} compilation in the runtime infrastructure supporting the execution of the application;
    \item[\bf GC optimization] refers to the support of \emph{garbage collection} in the runtime infrastructure supporting the execution of the application;
    \item[\bf Python version(s)] refers to the list of Python source code versions supported by the runtime environment.
\end{description}

We should explain the classification of these runtimes
\begin{table}
    \caption{Classification of Python implementations}
    \label{fig:python-classes}
    \center
    \begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
        \multirow{2}{*}{\bf Name} & \multirow{2}{*}{\bf Type} & \multirow{2}{*}{\bf Runtime} & \multicolumn{2}{c|}{\bf Optimisations} & \multicolumn{2}{c|}{\bf Python}                     \\
        % \cline{1-5} 
                                  &                           &                              & {\bf JIT}                              & {\bf GC}                        & {\bf 2} & {\bf 3} \\
        \hline
        \hline
        CPython                   & Interpreter               & C                            & \no                                    & \no                             & \yes    & \yes    \\
        \hline
        Intel\,Python             & Interpreter               & C                            & \no                                    & \no                             & \yes    & \yes    \\
        \hline
        ActivePython              & Interpreter               & C                            & \no                                    & \yes                            & \yes    & \yes    \\
        \hline
        PyPy                      & Interpreter               & Python                       & \yes                                   & \yes                            & \yes    & \yes    \\
        \hline
        IronPython                & Interpreter               & .Net                         & \yes                                   & \yes                            & \yes    & \yes    \\
        \hline
        GraalPython               & Interpreter               & GraalVM                      & \yes                                   & \yes                            & \no     & \yes    \\
        \hline
        Jython                    & Interpreter               & Java                         & \yes                                   & \yes                            & \yes    & \no     \\
        \hline
        Stackless\,Python         & Interpreter               & Python                       & \no                                    & \no                             & \yes    & \no     \\
        \hline
        MicroPython               & Interpreter               & c                            & \no                                    & \no                             & \no     & \yes    \\
        \hline
        Pyston                    & Interpreter               & LLVM                         & \yes                                   & \no                             & \yes    & \no     \\
        \hline
        Unladen\,Swallow          & Interpreter               & LLVM                         & \yes                                   & \no                             & \yes    & \no     \\
        \hline
        \hline
        Cython                    & Compiler                  & C                            & \no                                    & \no                             & \yes    & \yes    \\
        \hline
        Nuitka                    & Compiler                  & C                            & \no                                    & \no                             & \yes    & \yes    \\
        \hline
        Shedskin                  & Compiler                  & C++                          & \no                                    & \no                             & \yes    & \yes    \\
        \hline
        Grumpy                    & Compiler                  & Go                           & \no                                    & \no                             & \yes    & \yes    \\
        \hline
        \hline
        Numba                     & Library                   & C                            & \yes                                   & \no                             & \yes    & \yes    \\
        \hline
        Hope                      & Library                   & Python                       & \yes                                   & \no                             & \yes    & \yes    \\
        \hline
        Psyco                     & Library                   & Python                       & \yes                                   & \no                             & \yes    & \yes    \\
        \hline
        Pyjion                    & Library                   & .NET Core                    & \yes                                   & \no                             & \yes    & \yes    \\
        \hline
        Parakeet                  & Library                   & C                            & -                                      & \no                             & \yes    & \no     \\
        \hline
    \end{tabular}
\end{table}



\begin{table}[hbt]
    \caption{Classification of Python implementations}
    \label{fig:pythonimplementations}
    \center
    \begin{tabular}{|l|c|c|c|}
        \hline
        Version  & Interpreter     & Transpiler/Compiler & Jit library \\
        \hline
        \hline
        Python~2 & Cpython2        & Cython2             & Numba 2     \\
        % \cline{2-4} 
                 & Pypy2           & Shesdskin           & Hope        \\
        % \cline{2-4} 
                 & Pytson          & Grumpy              & Parakeet    \\
                 & Ironpython      &                     & Psyco       \\
                 & Jython          &                     & Pyjion      \\
        % &  &  &  \\
                 & Micropython     &                     &             \\
                 & Pysec           &                     &             \\
                 & StacklessPython &                     &             \\
        % \cline{2-4} 
        \hline
        Python~3 & Cpython3        & Nuitka              & Numba3      \\
                 & Pypy3           &                     &             \\
                 & GraalPython     &                     &             \\
        \hline
    \end{tabular}
\end{table}

There are other implementations that we did not consider because either the project aborted many years ago or it has very limited support for Python features.
After the collection of those implementations, we filtered them. To keep only the versions that are still maintained and support most Python features.
and we classified them into 3 categories depending on their integration with the python code.
In \Cref{fig:pythonimplementations}, we describe the implementations that we kept and the version of each implementation and its category.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/python-implementations-tree}
    \caption{Python interpeters}
    \label{fig:interpreters}
\end{figure}



\section{experimetal protocole}

As we have discussed in the previous chapter \ref{chapter:benchmarking}. instead of running the tests, the idea was to design a system that allows practiouners to reproduce and extends our tests. and then we use the same system to run answer some of researchs question.
%%%% probably ill resue the same things from other chapters 
\subsection{measurement context}
\paragraph{Hardware settings}

all our tests have been executed in a Dell PowerEdge C6420 server machine. A summary of its hardware is listed in table \ref{fig:dahuconfig}. The machine is equiped with a minimal version of Debian\,9 (4.9.0 kernel version) where we install Docker (version 18.09.5).



\begin{table}[hbt]
    \begin{tabular}{ll}
        \hline
        CPU     & Intel Xeon Gold 6130 (Skylake, 2.10GHz, 2 CPUs/node, 16 cores/CPU)                                                     \\
        Memory  & 192 GiB                                                                                                                \\
        Storage & 240 GB SSD SATA Samsung MZ7KM240HMHQ0D3                                                                                \\
                & 480 GB SSD SATA Samsung MZ7KM480HMHQ0D3                                                                                \\
                & 4.0 TB HDD SATA Seagate                                                                                                \\
        Network & eth0/enp24s0f0, Ethernet, configured rate: 10 Gbps, model: Intel Ethernet Controller X710 for 10GbE SFP+, driver: i40e \\
                & ib0, Omni-Path, configured rate: 100 Gbps, model: Intel Omni-Path HFI Silicon 100 Series [discrete], driver: hfi1      \\
        \hline
    \end{tabular}
    \caption{Testing Machine Configuration }
    \label{fig:dahuconfig}
\end{table}

\begin{figure}[hbt]
    \includegraphics[width=\linewidth]{imgs/SmartWatts.png}
    \caption{powerapi architecture}
    \label{fig:powerapi}
\end{figure}

\paragraph{software settings}
For the sake of reproducibility, each experiment runs within a Docker container.
% for each test we create a docker image.
\subsection{Metrics}
Our focus will be mainly on CPU energy consumption because it is ten folds more than the DRAM one, since it is finit job benchmarking, time is highly correlated within the energy, and it will be only useful to explain certain energytical behaviour so we wont put a lot of focus on this metric.

%% basically i what i want to say is that we we care mostly about energy, because there are studies that were made on the performance, plus the DRAM is not that interested even within the heavy memory  benchmakrs 
% TODO : ADD DRAM graphs for vectors and matrix multiply 


\paragraph{Energy measurement}

As we know, the energy of a program is the integrale of its power overtime. For us ower case, we used Intel \emph{Running Power Average Limit} (RAPL)~\cite{Khan:2018:RAE:3199681.3177754} to collect the power samples of the running tests,

We used used \textsc{PowerAPI}~\cite{DBLP:journals/jss/ColmantRKSFS18}, to report Data collected by intel Rapl and send it into another machine that we call computing machine. then we calculate the Energy using  the the trapezoidal rule.


\ref{fig:trapezrule}
\begin{figure}[hbt]
    \centering
    \begin{equation}
        E = \int^a_b P(t)dt \simeq \sum^n_{k=1} \frac{P(t_k-1)+P(t_k)}{2}
    \end{equation}
    \caption{}
    \label{fig:trapezrule}
\end{figure}



figure \ref{fig:powerapi} shows the architecture of our testing model.


The reason of separation between data collection and energy calculation is to minimise any interference with the test so our sensor is a light c program running inside a docker container.
\subsection{tests preparation}



To study the behaviour of the python implementation regarding the energy consumption, we have to focus on the effect of the implementation and mitigate the maximum any side effects such as the organisation of the code or any extra consumption due to the operating system or tier libraries.
Therefore, for each test we took the implementation written in python2 as a reference and tried to use it in other implementations as it is. If it is not supported by python3, we transformed the code using the officiel library  \emph{2to3}\footnote{\url{https://docs.python.org/3.7/library/2to3.html}}.
In the case of the libraries that use \emph{JIT} adding a decorator to the function that we want to optimize was enough, if there are other changes we assume that they alter the original code which is against our purpose.

Each test is implemented in a Docker container for the several reasons
\begin{itemize}
    \item Isolation: each container has only the test program implemented with a single python runtime to remove any interference between different implementations.
    \item Deplpoyment: to use the testing machine without extra configurations that may alter the behaviour of the os toward the enrgy consumtion
    \item Reproductibility: One of the most frequent benchmark crimes~\cite{DBLP:journals/corr/abs-1801-02381} in research is the lack of Reproductibility, by using Docker we ensure that each test has an Image that will be accessible in public.
\end{itemize}

Despite the presence of the official docker images for the most of the runtimes, we prefered to build our own using the same reference image in order to remove any bias due to the Os used in the offical image.We used ArchLinux with the kernel version 4.9.184 as a base image.


\subsection{Extension}
As we have done with the previous chapters. we provide a tool that allows to extend the tests with new workloads and new candidates.
In the repository \footnote{\url{https://github.com/chakib-belgaid/python-implementations}}.
we have a tool that allows to generate new workloads and new candidates.
The script \texttt{generator.py} allows to create new benchmarks by implementing a python code within different interpeters. then it generates \texttt{launcher-benchmark.sh} that can be used directly to run the expermient. Furthermore all the successful implementations are stored in seperate directory and the that couldn't work ( mostly because of compatibility issues) stored in a recap file called \texttt{benchmarkTest.md}. where benchmark is the name of the new workload;
To add extra \textbf{candidates} one should add a base docker file
that containes the new implementation and if there should be extra manipulation that should be added to the workload files ,such as adding new decorator or changing some parameters, then they should be added as an extra function in the script \texttt{generator.py}. Finally they should be included in the python candidates.

\section{Results and finding}

\subsection{Preliminary stduies}
This part will be dedicated to analyse the initial behaviour of different python interpereters.
First we started with a typical benchmark \em binarrytree. where we compare the energy consumption of the different implementations.


\section{placeholder}

we performed a shapiro normality test for the first on the results, and almost all the p-values smaller than alpha=0.01\% therefore the distrubution is not normal

for arrays and vectors  graalpython took so much time that we decided to remove it

the startup cost for ironpython was too high
for graalpython and ipy they couldnt handle big numbers hence the overflow  when it comes to execution time
but since we measure the energy outside the values of the energy aren't impacted

as usual there is  correlaction between DRAM and CPU so no need to classify based on DRAM + the DRAM consumes less 10\% of the  energy compared to the CPU

so the plan is to prove that there is a statistical difference
then we use the multiparameter optimization , howerver we stop in the phase where we calculate the score
because we dont know the weight of each parameter ( of tommti microbenchmark) and instead of doing the sum with the coefissients we use the radar plot to let the reader decide which one is the most suitable for his usage. howerver since the differences are gigantics we change the scale to logarithmic to be easier for the eye to read



\begin{figure}

    \label{table:bitops}
    \begin{tabular}{|lrrrrr|}
        \toprule
        benchmark    & A       & GG      & LL      & Or      & XOR     \\
        \midrule
        activepython & 676.980 & 763.208 & 651.783 & 743.016 & 728.828 \\
        cpython2     & 441.082 & 435.886 & 430.846 & 415.247 & 419.081 \\
        cpython3     & 595.209 & 685.085 & 563.839 & 657.972 & 655.560 \\
        cython       & 35.077  & 182.688 & 274.177 & 34.868  & 34.504  \\
        nuitka       & 33.260  & 32.980  & 33.256  & 33.472  & 33.030  \\
        numba2       & 9.102   & 8.411   & 9.460   & 9.375   & 9.755   \\
        numba3       & 9.566   & 10.144  & 9.219   & 9.344   & 9.665   \\
        pypy2        & 8.456   & 7.844   & 8.286   & 8.138   & 7.952   \\
        pypy3        & 7.552   & 8.093   & 8.108   & 8.669   & 8.623   \\
        shedskin     & 8.024   & 8.070   & 8.399   & 8.126   & 8.277   \\
        \bottomrule
    \end{tabular}
    \vfill
    \centering
    \includegraphics[width=\linewidth]{imgs/bitopts_mean}
    \caption{energy consumption of different impelementations using Bit Operation benchmarks (Joule) }
\end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{imgs/barplot_binarry_tree}
%     \caption{energy behaviour of different python implementations for the binarry Tree benchmark}
%     \label{fig:binarrytree}
% \end{figure}

\begin{figure}

\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/dendogram_interpreters}
    \caption{Dendogram of the different implementations}
    \label{fig:dendogram}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/alltomti_performance}
    \caption{different interpererts optimisation }
    \label{fig:tommi_all}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/tommti_compare__pypy2_pypy3}
    \caption{green factor of pypy }
    \label{fig:pypy2vspypy3}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/tommti_compare__cpython3_numba3_pypy3}
    \caption{comparaison of pypy vs python vs numba }
    \label{fig:p3}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/tommti_compare__cpython2_pypy2_numba2}
    \caption{comparaison of pypy vs python vs numba }
    \label{fig:p2}
\end{figure}


As we can see in figure \ref{fig:tommi_all}, there is no evolution between cpython2 and cpython3
Intelpython and active python both folows the same behaviour. one can conclude that the work that have been on those interpreters is mainly
to improve a specific purpose, Active python claims that their version is focused on secuirty which explains the lack of some performances due to the introduction of more reonforcment
.Morver Intel published their version of python as a dedicated for machine learning. Unfortunately the tommti benchmark is a set that focuses on the general purpose programming
which does not reflect the performance of the machine learning.
Another aspect of a such behaviour might be due to the fact of the processors that were used in the testbed. and it might change in the future if we include the GPU part for the machine learning benchmarks
% reference the work of the intern 
As for nuitka. there were no optimization in the energy consumpotion despite the fact that is it a complier.
However if we dig through the nuitka mechanisms. they basically embed the python code with an interpreter.
Unllike nuitka shedskin exhebit a the best energy consumption pattern when it comes to the arithmetic operartions. One can conclude it is due to the fact of the native type of the variables. unlike the interpreters where they are treated as object in the begining.

for the other interpeters pypy is very promising especially when it comes to data maniupulation as one can see in the figure \ref{fig:bar_tommtei_vector}
pypy is by far the best interperer when it comes to treating vectors.
numba2 introduced the JIT but wasn't as promising as numba3.

for the other vm based interpereters. jython and ipy lacked in term of energy optimisation which was kinda expected since they were in their the begining of the stage and the main puprose of such implementation is to link the bytecode generated by jython and ironpython with their respectives virtual machin.

Unlike the the previous interpeters. graal exhebits a certin promises when it comes to complex algorithms - nested loops -
micro python is dedicated to embeded systems so lunching it is powerful cpu machines will be misleading.

Most of the interperters had the same behaviour when it comes to the input outputs. except for jython which was kinda of a anomaly probably due to the lack of the optimizations.
% pypy handles exceptions very well 


\section{conclusion}
One may observe that the choice of Python interpreter has a significant impact on the programs' energy consumption.
This investigation is made more intriguing by the absence of a universal solution.
The primary downside is the incompatibility of some of these solutions, which causes us to make concessions when we need a generic answer.



\input{table-envs.tex}