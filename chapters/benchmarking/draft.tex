\section{Reproducibility within the context of energy}\label{sec:benchmarking_reproducibility}
\subsection{Introduction}

Empirical measurements are critical to capture the effect of developers' choices on software energy consumption.
To accomplish this, one should not overlook the benchmarking pitfalls highlighted in the state of the art~\cite{van_der_kouwe_benchmarking_2018}.

Second, one should not ignore tremendous progress in computer science, which has led to a rise in the number of obsolete results.
Furthermore, when it comes to comparative research, the execution environment may impact the study itself.
Finally, between the exploratory experiment and the publication of the results, new candidates may have emerged, and others may have changed.

As a result, it is critical to ensure that the results can be \emph{reproduced}, so one can test their hypothesis in different environments and provide room for new candidates.
In this section, we will address these issues and investigate several ways of encapsulating the systems-under-test to ensure experiment reproducibility in the context of energy consumption tests while providing the benefits and drawbacks of each strategy.
Later in this section, we will propose a protocol to ensure that the results are reproducible and extensible.

% Furthermore, when it comes to evaluating the energy consumption of software, other challenges occur, such as the impact of hardware, the difference between the machines used for measurements and those that may apply the results, and so on.
% Regarding the "representativeness" of the benchmarks, we will elaborate in the section entitled Perspectives.
%% this is might be the part that treats reproducibility for my case 



%%%%%%%%% preliminary taughts and section structure 
% The need of reproducibility in our field - software optimization based on empirical studies -

% The importance of Virtualisation for reproducibility \cite{howe_virtual_2012}
% some of the most important parts are
% - fewer constraints on research methods
% - on-demand backups
% - virtual Machines as Publications
% - more Variables captured



%%%% part of the state of the art 
% In the area covered by this PhD thesis, reproducibility might be achieved by ensuring the same execution settings of physical nodes, virtual machines, clusters or cloud environments.
%%% why it won't work for energy consumption 
% However, when it comes to measuring the energy consumption of a system, applying acknowledged guidelines and carefully repeating the same benchmark can nonetheless lead to different energy footprints not only among homogeneous nodes, but even within a single node.
%% reseasons why 
% One major problem that hinders the reproducibility of the empirical benchmarks is the interaction with the external environment, either as concurrency or dependencies.
% Therefore, researchers cannot observe the same results, unless they duplicate the same environment.


% \begin{itemize}
%     \item textbf{expermimental protocol}: Which describes, the context and  an orchestrator that handles the experiment, this orchestrator should take as an entry 
%     \item \textbf{candidates}: a set of candidates that will be used in the experiment. The candidates should be agnostic of the experiment context and they all
% \end{itemize}

% Now that we have covered the general strucutre of the protocol, this section details the specific aspects of the protocol that are relevant to the energy consumption analysis.
% We will first discuss how to insure the extension of the candidates. later we will talk about the energy metrics and how to ensure the accuracy when it comes to the measurement of the energy consumption.


\subsection{Virtual Machines}
The first choice should be to use \emph{Virtual Machines} (VM). This technology enables researchers to choose the most appropriate tools, software, and operating system for their needs without incurring the cost of changing the working environment, giving them more control over dependencies and the execution environment.
Furthermore, adopting a VM addresses the \emph{replication crisis} since virtual images allow even the most sophisticated architecture to be replicated by instantiating a copy of the image.

This option, however, comes at a cost. Because of the hypervisor, the software will be built on two kernels: one for the virtual machine (guest) and one for the host machine, resulting in a visible overhead and a negative influence on the performance of the system-under-test.
As a result, we cannot use VM for performance-related tests.
Isolation is another drawback of VM: while this feature protects the experimental setting from unwanted interference from the outside world, this interaction may be requiredâ€”-especially if the experiment is dependent on an external source such as energy monitors.

\subsection{Containers}
Another option would be to use something that allows us to benefit from the host OS's isolation while simultaneously simplifying replication as proposed by VM and the direct interface with the hardware provided by the traditional techniques.

Containers provide such an advantage by ensuring application separation and ease of replication.
Figure~\ref{environement:virtualization_technique} depicts the architectural differences between virtualization and container technologies. There are three main types of virtualization.
\begin{itemize}
    \item \textsf{Type\,1}: runs on the hardware directly.
          It is primarily utilized by cloud providers with no host OS and only VMs that run on the open-source Xen or VMware ESX hypervisors.

    \item \textsf{Type\,2}: runs on top of the host operating system and is most commonly found on personal computers. VMware servers and VirtualBox are notable examples of this category, and most researchers' experiments use them.
          However, the applications are typically slower because of the two operating systems.

    \item \textsf{Containers}: run their operating systems on the host kernel rather than their own, which makes them smaller, faster, and more efficient in terms of hardware utilization. One can cite \emph{Docker}, \emph{Linux LXC}, or \emph{LXD}~\cite{abuabdo_virtualization_2019}.
\end{itemize}


\begin{figure}
    \center{\includegraphics[width=1\linewidth]{imgs/virtualization_techniques}}
    \caption{Different Methods of Virtualization}\label{environement:virtualization_technique}
\end{figure}


\subsection{Docker Vs. Virtual Machine}
Despite the fact that Type 1 is more performant than Type 2, the latter is the most used in research, as most researchers tend to conduct their experiments on their own machines. Docker, on the other hand, is the most well-known container technology. In our case, we are more likely to promote Docker for two reasons:
\begin{enumerate}
    \item As previously stated in literature~\cite{van2016power,morabito_power_2015}, we require a lightweight orchestrator to limit the overhead on the energy usage of our experiments
    \item We need to communicate with the host OS because we are using hardware sensors to measure energy consumption.
\end{enumerate}

% A special notice to \href{https://github.com/powerapi-ng/virtualwatts}{virtualwatts}. A framework that enables us to retrieve the energy consumption of a virtual machine.


\subsection{Docker \& Energy}
Now that we have decided to use container technology to enclose our tests, what effect will this have on the amount of energy consumed by our tests?
%%%%%%%%%


Using research from \cite{eddie_antonio_santos_how} who examined how adding the Docker layer affected energy consumption, \citeauthor{eddie_antonio_santos_how} conducted their experiment by running numerous benchmarks both with and without Docker. They contrasted the energy usage and execution time that resulted.
The first step was to observe the effects of the orchestrator and the Docker daemon while there was no work to be done.
Then, they used three benchmarks in their experiments: WordPress, Redis, and PostgreSQL.
The values below show the system under test's energy consumption while it is idle.
Docker has an overhead of roughly $1,000$  joules, as seen in Figure~\ref{fig:docker_idle}.


\begin{figure}
    \center{\includegraphics[width=.5\linewidth]{imgs/docker_vs_vm_energy_paper/idle_energy}}
    \caption{energy consumption of Idle system with and without docker \cite{eddie_antonio_santos_how}}\label{fig:docker_idle}
\end{figure}

However, as seen in Figure~\ref{fig:docker_reddis}, Docker increased the execution duration of the benchmark by 50 seconds, which led to a significant rise in energy usage.
According to the authors, the Docker daemon is primarily responsible for this overhead, not the fact that the application is running in a container.


Furthermore, they calculated the cost of this extra energy, which was less than 0.15\$ in the worst-case scenario, which is insignificant compared to the benefits of Docker for isolation and reproducibility.

To summarize, Docker-based software consumes more energy since it takes longer to execute.
The execution of the Docker daemon causes an increase in average power consumption of only \textbf{2\,Watts}.
This overhead can reach up to 5\% in IO-intensive applications, while it is barely visible in CPU- or DRAM-intensive workloads.

\begin{figure}[!bht]
    \includegraphics[width=.5\linewidth]{imgs/docker_vs_vm_energy_paper/reddis_time}
    \includegraphics[width=.5\linewidth]{imgs/docker_vs_vm_energy_paper/reddis_energy}
    \caption{Execution time \& energy consumption of Redis with and without Docker~\cite{eddie_antonio_santos_how}}\label{fig:docker_reddis}
\end{figure}

As can be seen, the addition of the supervisor has increased Docker's impact on energy, which will be distributed equitably across all experiments.
Therefore, when it comes to comparison analysis, it will mitigate its impact automatically.
Furthermore, because we have access to the host hardware, we do not need to worry about capturing the SUT's energy use.
As a result, we will use Docker to keep all of our tests separate, ensuring that they can be repeated clearly and simply.

\subsection{Extension}
% TODO : proofread
\subsubsection{Definition of Extention}
With the rapid evolution of the software industry, Even ensuring the reproducibility of the same research will not be enough.
Each day, new software versions are released, and new features are added. Even more, the goal of the research might evolve. As a result, nowadays, especially in comparative studies, it is essential to leave room for expansion.

One can expand their experiment through three axes :
\begin{itemize}
    \item \textbf{proposed solutions}: Where one will expand their research by including additional solutions and comparing them to earlier ones
    \item \textbf{evaluation criteria}: This axe's objective is to broaden the evaluation criteria to incorporate additional measures like performance, cost, and security, among others.
    \item \textbf{benchmarks}: This axe aims to enlarge the benchmarks to include others to broaden the research scope.
\end{itemize}

\subsubsection{The architecture of the extension}
\begin{figure}
    \center{\includegraphics[width=\linewidth]{imgs/benchmarkingprotocol}}
    \caption{Benchmarking protocol}\label{fig:benchmarkingprotocol}
\end{figure}
To be able to extend the empirical experiments through these axes,
We propose to enhance the benchmarking framework suggested by the \emph{Collaboratory on Experimental Evaluation of Software and Systems in Computer Science}\footnote{\url{http://evaluate.inf.usi.ch/}}.
Instead of only presenting the four primary aspects of their guidelines that were mentioned previously in Section~\ref{sec:benchmarking_reproducibility}, we suggest an abstract model describing an empirical experiment.
Figure~\ref{fig:benchmarkingprotocol} shows the proposed model while comparing the existing solution and the proposed one.

The model is composed of different components that are described below,
\paragraph{Context}
The hardware and the software configuration for the actual experiment, the purpose of this part is to provide extra information to help readers better judge the results and reproduce the experiment while diminishing the impact of external factors.

\paragraph{orchestrator}

The core design of the experiment is responsible for running the experiment regardless of the context.
In the experiment, it is the only component that is allowed to interact with the SUT.
The orchestrator provides three interfaces:
\begin{itemize}
    \item \emph{Workload interface}: provides a set of functions that the workload should implement to be called by each candidate in the experiment. This interface is responsible for extending the experiment with new benchmarks.
    \item \emph{Observer interface}: provides a set of metrics that the orchestrator collects during the experiment. Implementing the observer interface allows the user to extend the experiment with new metrics.
    \item \emph{The Candidate interface}: It provides a set of functions that the candidate should implement to called  by the orchestrator. This interface is responsible for extending the experiment with new solutions.
\end{itemize}

\paragraph{Data}
The row data collected by the experiment. In our case, it will be provided by the \emph{observator}. One or more observators can be used to collect different metrics. This data aims to ensure the \emph{replication}, allowing the reader to perform extra analysis without executing the experiment a second time.


\paragraph{Analysis}
The final part should provide the set of methods and functions used to do the empirical analysis to answer the research questions.



% It starts with the orchestrator is the core design of the experiment  for the coordination between the workload and the candidate, therefore it provides an interface that is implemented by each workload.
% Later this interface will be provided to each candidate by the orchestrator. Like this we will remove any bias of dependency between the workload and the candidates, which will alow us to extend both parts without any change in the code.
% Instead of retrieving the data directly from the experiment, we suggest to use an observator to allow the extension of the metrics, this observer in general should be outside the experiment to be able to gather as many informations without impacting the the code.









\subsection{Conclusion}

In this section, we addressed the first challenge of empirical research, which is the \textbf{reproducibility} of the experiments.
We started by listing the different options for encapsulating the system under test.
Then, we have shown that using VMs is unsuitable for performance or energy-related tests, while containers are a good alternative. Later, we discussed the benefits and drawbacks of using Docker for energy consumption experiments. We have demonstrated that it has a constant overhead that is self-mitigated when compared.
Lastly, we have proposed an addition to the benchmarking framework that would allow the experiment to be extended along the three axes already mentioned.



\clearpage