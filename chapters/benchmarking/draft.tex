
\section{Motivation}
Empirical measurements are critical to capture the effect of developers' choices on software energy consumption.
To accomplish this, one should not overlook the benchmarking pitfalls highlighted in the state of the art~\cite{van_der_kouwe_benchmarking_2018}.
Furthermore, when it comes to evaluating the energy consumption of software, other challenges occur, such as the impact of hardware, the difference between the machines used for measurements and those that may apply the results, and so on.
Moreover, considerable advances in computer science have resulted in an increase in the number of obsolete results.
Addressing all of them becomes significantly more difficult in studies that compare multiple treatments.

Furthermore, between the exploratory experiment and the published results, there may have been the appearance of new candidates as well as the evolution of others.
As a result, it is not only necessary to ensure the "reproducibility" of the results, but it is also essential to provide room for new candidates.
As a result, we'd like to present "Extension", a new approach for leveraging experimentation.
The ability to give not only the tools needed to replicate the tests but also to integrate more candidates, workloads, or important key performance indicators

This chapter covers ways to overcome empirical analysis challenges in energy consumption studies. First, we go through the three components of a successful benchmark within the energy consumption experiment. Section~\ref{sec:benchmarking_reproducibility} concentrates on the "reproducibility" issue and how to deliver reproducible experiments without interfering with the energy measurement. Section~\ref{sec:taming-the-energy-variation} discusses accuracy in software energy consumption. 
Finally, Section~\ref{sec:bench_extension} presents tools for introducing new candidates, workloads, or other key performance indicators.



Regarding the "representativeness" of the benchmarks, we will elaborate in the section entitled Perspectives.
%% this is might be the part that treats reproducibility for my case 



%%%%%%%%% preliminary taughts and section structure 
% The need of reproducibility in our field - software optimization based on empirical studies -

% The importance of Virtualisation for reproducibility \cite{howe_virtual_2012}
% some of the most important parts are
% - fewer constraints on research methods
% - on-demand backups
% - virtual Machines as Publications
% - more Variables captured



%%%% part of the state of the art 
% In the area covered by this PhD thesis, reproducibility might be achieved by ensuring the same execution settings of physical nodes, virtual machines, clusters or cloud environments.
%%% why it won't work for energy consumption 
% However, when it comes to measuring the energy consumption of a system, applying acknowledged guidelines and carefully repeating the same benchmark can nonetheless lead to different energy footprints not only among homogeneous nodes, but even within a single node.
%% reseasons why 
% One major problem that hinders the reproducibility of the empirical benchmarks is the interaction with the external environment, either as concurrency or dependencies.
% Therefore, researchers cannot observe the same results, unless they duplicate the same environment.


% \begin{itemize}
%     \item textbf{expermimental protocol}: Which describes, the context and  an orchestrator that handles the experiment, this orchestrator should take as an entry 
%     \item \textbf{candidates}: a set of candidates that will be used in the experiment. The candidates should be agnostic of the experiment context and they all
% \end{itemize}
\section{Reproducibility within the context of energy}\label{sec:benchmarking_reproducibility}
% Now that we have covered the general strucutre of the protocol, this section details the specific aspects of the protocol that are relevant to the energy consumption analysis.
% We will first discuss how to insure the extension of the candidates. later we will talk about the energy metrics and how to ensure the accuracy when it comes to the measurement of the energy consumption.
This section discusses various techniques to encapsulate the systems-under-test in order to assure the reproducibility of an experiment.

\paragraph{Virtual Machines}
The very first choice should be to use \emph{Virtual Machines} (VM), which allow researchers to select the most appropriate tools, software, and operating system that they are most comfortable with without incurring the cost of changing the actual working environment, giving them more control over dependencies and the execution environment.
Furthermore, adopting a VM addresses the \emph{replication crisis} since virtual images allow even the most sophisticated architecture to be simply replicated by instantiating a copy of the image.

This option, however, comes at a cost. Because of the hypervisor, software will be built on two kernels: one for the virtual machine (guest) and one for the host machine, resulting in a visible overhead and a negative influence on the performance of the system-under-test. As a result, we cannot use VM for performance-related tests. Isolation is another drawback of VM. While this feature protects the experimental setting from unwanted interference from the outside world, it is possible that this interaction is requiredâ€”-especially if the experiment is dependent on an external source such as energy montitors .

\paragraph{Containers}
Another option would be to use something that allows us to benefit from the host OS's isolation while simultaneously simplifying replication as proposed by VM and the direct interface with the hardware provided by the traditional techniques.

Containers provide such an advantage by ensuring application separation and ease of replication.
Figure~\ref{environement:virtualization_technique} depicts the architectural differences between virtualization and container technologies. There are three main types of virtualisation.
\begin{itemize}
    \item \textsf{Type\,1}: runs on the hardware directly.
          It is primarily utilized by cloud providers where there is no host OS and only VMs that run on the open-source Xen or VMware ESX hypervisors.

    \item \textsf{Type\,2}: runs on top of the host operating system and is most commonly found on personal computers. VMware server and virtualBox are notable examples of this category, and most researchers' experiments use them.
          However, because of the two operating systems, the applications are typically slower.

    \item \textsf{Containers}: run their operating systems on the host kernel rather than their own , which makes them smaller, faster, and more efficient in terms of hardware utilization. One can cite \emph{Docker}, \emph{Linux LXC}, or \emph{LXD}~\cite{abuabdo_virtualization_2019}.
\end{itemize}


\begin{figure}
    \center{\includegraphics[width=1\linewidth]{imgs/virtualization_techniques}}
    \caption{Different Methods of Virtualization}\label{environement:virtualization_technique}
\end{figure}


\subsection{Docker Vs. Virtual Machine}
Despite the fact that Type 1 is more performant than Type 2, the latter is the most used in research, as most researchers tend to conduct their experiments on their own machines. Docker, on the other hand, is the most well-known container technology. In our case, we are more likely to promote Docker for two reasons:
\begin{enumerate}
    \item As previously stated [143] [reference Morabito (2015) and van Kessel et al. (2016)], we require a lightweight orchestrator to limit the overhead on energy usage of our experiments
          \cite{van2016power}
          % TODO : ask romain about the title of the paper and the reference to the paper
          [cite Morabito (2015) and van Kessel et al. (2016)],
          % cite Power efficiency of hypervisor-based virtuali- zation versus container-based virtualization. University of Amsterdam.
    \item We need to communicate with the host OS because we are using hardware sensors to measure the energy consumption.
\end{enumerate}

%  TODO : Rephrase this 
% A special notice to \href{https://github.com/powerapi-ng/virtualwatts}{virtualwatts}. A framework that enables us to retrieve the energy consumption of a virtual machine.


\subsection{Docker \& Energy}
Now that we have decided to use container technology to enclose our tests, what effect will this have on the amount of energy our tests use?
%%%%%%%%%

Using research from \cite{santos2018does} who examined how adding the Docker layer affected energy consumption. \citeauthor{santos2018does} conducted their experiment by running numerous benchmarks both with and without Ddocker. They contrasted the energy usage and execution time that resulted.
The first step was to observe the effects of the orchestrator and the Docker deamon while there was no work to be done.
Then, they use three benchmarks in their experiments: Wordpress, Reddis and PostgreSQL.
The values below show the system under test's energy consumption while it is idle.
Docker has an overhead of roughly $1,000$  joules, as seen in Figure~\ref{fig:docker_idle} .


\begin{figure}
    \center{\includegraphics[width=.5\linewidth]{imgs/docker_vs_vm_energy_paper/idle_energy}}
    \caption{energy consumption of Idle system with and without docker \cite{santos2018does}}\label{fig:docker_idle}
\end{figure}

Yet, as seen in Figure~\ref{fig:docker_reddis}, Docker increased the execution duration of the benchmark by 50 seconds,which led to a significant rise in the energy usage.
According to the authors, this overhead is primarily caused by the Docker deamon and not by the fact that  the application is in a container.


Furthermore, they calculated the cost of this extra energy, which was less than 0.15\$ in the worst-case scenario, which is insignificant in comparison to the benefits of Docker for isolation and reproducibility.

To summarize, Docker-based software tends to consume more energy since it takes longer to execute.
The execution of the Docker deamon causes an increase in average power consumption of only \textbf{2\,Watts}.
This overhead can reach up to 5\% in IO-intensive applications, while it is barely visible in CPU- or DRAM-intensive workloads.



\begin{figure}[!bht]
    \includegraphics[width=.5\linewidth]{imgs/docker_vs_vm_energy_paper/reddis_time}
    \includegraphics[width=.5\linewidth]{imgs/docker_vs_vm_energy_paper/reddis_energy}
    \caption{Execution time \& energy consumption of Redis with and without Docker~\cite{santos2018does}}\label{fig:docker_reddis}
\end{figure}


\subsection{Conclusion}
As can be seen, the introduction of the supervisor has increased the impact of Docker on energy that will be applied equitably to all experiments.
Therefore when it comes to comparison analysis, it will mitigate its impact automatically.
Furthermore, because we have access to the host Hardware, we do not need to worry about capturing the SUT's energy use.
As a result, we will use Docker as an encapsuation solution for all of our experiments.
It will ensure the reproducibility of our experiments in a simple and straightforward manner.
\clearpage
